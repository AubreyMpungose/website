---
title: "SLR: Model diagnostics"
subtitle: "STA 210 - Spring 2022"
author: "Dr. Mine Ã‡etinkaya-Rundel"
footer: "[sta210-s22.github.io/website](https://sta210-s22.github.io/website/)"
logo: "images/logo.png"
format: 
  revealjs:
    theme: slides.scss
    multiplex: true
    transition: fade
    slide-number: true
    incremental: true 
    chalkboard: true
editor: visual
execute:
  freeze: auto
---

```{r setup}
#| include: false

library(countdown)

knitr::opts_chunk$set(
  fig.width = 8,
  fig.asp = 0.618,
  fig.retina = 3,
  dpi = 300,
  out.width = "80%",
  fig.align = "center"
)

options(scipen = 100)
```

# Welcome

## Computational setup

```{r packages}
#| echo: true
#| message: false

# load packages
library(tidyverse)   # for data wrangling and visualization
library(tidymodels)  # for modeling
library(openintro)   # for the duke_forest dataset
library(scales)      # for pretty axis labels
library(knitr)       # for pretty tables
library(kableExtra)  # also for pretty tables
library(patchwork)   # arrange plots

# set default theme and larger font size for ggplot2
ggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))
```

# Mathematical models for inference

## The regression model, revisited

```{r}
#| echo: true

df_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(price ~ area, data = duke_forest)

tidy(df_fit) %>%
  kable(digits = 2)
```

## HT for the slope {.smaller}

**Hypotheses:** $H_0: \beta_1 = 0$ vs. $H_A: \beta_1 \ne 0$

. . .

**Test statistic:** Number of standard errors the estimate is away from the null

$$
T = \frac{\text{Estimate - Null}}{\text{Standard error}} \\
$$

. . .

**p-value:** Probability of observing a test statistic at least as extreme (in the direction of the alternative hypothesis) from the null value as the one observed

$$
p-value = P(|t| > |\text{test statistic}),
$$

calculated from a $t$ distribution with $n - 2$ degrees of freedom

## HT: Test statistic

```{r}
tidy(df_fit) %>%
  kable(digits = 2) %>%
  row_spec(2, background = "#D9E3E4")
```

$$
t = \frac{\hat{\beta}_1 - 0}{SE_{\hat{\beta}_1}} = \frac{159.48 - 0}{18.17} = 8.78
$$

## HT: p-value

::: columns
::: {.column width="70%"}
```{r}
tidy(df_fit) %>%
  kable(digits = 2) %>%
  row_spec(2, background = "#D9E3E4")
```
:::

::: {.column width="30%"}
```{r}
#| out.width: "100%"

normTail(L = -8.78, U = 8.78, df = nrow(duke_forest) - 2, xlim = c(-9,9), col = "#D9E3E4")
```
:::
:::

## Understanding the p-value

|  Magnitude of p-value   |            Interpretation             |
|:-----------------------:|:-------------------------------------:|
|     p-value \< 0.01     |     strong evidence against $H_0$     |
| 0.01 \< p-value \< 0.05 |    moderate evidence against $H_0$    |
| 0.05 \< p-value \< 0.1  |      weak evidence against $H_0$      |
|     p-value \> 0.1      | effectively no evidence against $H_0$ |

::: callout-important
These are general guidelines.
The strength of evidence depends on the context of the problem.
:::

## HT: Conclusion, in context

```{r}
tidy(df_fit) %>%
  kable(digits = 2) %>%
  row_spec(2, background = "#D9E3E4")
```

-   The data provide convincing evidence that the population slope $\beta_1$ is different from 0.
-   The data provide convincing evidence of a linear relationship between area and price of houses in Duke Forest.

## CI for the slope

$$
\text{Estimate} \pm \text{ (critical value) } \times \text{SE}
$$

. . .

$$
\hat{\beta}_1 \pm t^* \times SE_{\hat{\beta}_1}
$$

where $t^*$ is calculated from a $t$ distribution with $n-2$ degrees of freedom

## CI: Critical value

::: columns
::: {.column width="60%"}
```{r}
#| echo: true

# confidence level: 95%
qt(0.975, df = nrow(duke_forest) - 2)

# confidence level: 90%
qt(0.95, df = nrow(duke_forest) - 2)

# confidence level: 99%
qt(0.995, df = nrow(duke_forest) - 2)
```
:::

::: {.column width="40%"}
```{r}
#| out.width: "100%"

normTail(M = c(-1.984984, 1.984984), df = nrow(duke_forest) - 2, col = "#D9E3E4")
text(x = 0, y = 0.04, labels = "95%", cex = 2, col = "#5B888C")
```
:::
:::

## 95% CI: Calculation

```{r}
tidy(df_fit) %>% 
  kable(digits = 2) %>%
  row_spec(2, background = "#D9E3E4")
```

$$\hat{\beta}_1 = 159.48 \hspace{15mm} t^* = 1.98 \hspace{15mm} SE_{\hat{\beta}_1} = 18.17$$

. . .

$$
159.48 \pm 1.98 \times 18.17 = (123.50, 195.46)
$$

## 95% CI: Computation {.smaller}

```{r}
#| echo: true

tidy(df_fit, conf.int = TRUE, conf.level = 0.95) %>% 
  kable(digits = 2)
```

# Model conditions

## Model conditions

1.  **Linearity:** There is a linear relationship between the response and predictor variables
2.  **Constant variance:** The variability of the errors is equal for all values of the predictor variable, i.e. the errors are homeoscedastic
3.  **Normality:** The errors follow a normal distribution
4.  **Independence:** The errors are independent from each other

## Linearity

âœ… The residuals vs. fitted values plot should not show a random scatter of residuals (no distinguishable pattern or structure)

```{r res-vs-fit}
df_aug <- augment(df_fit$fit)

ggplot(df_aug, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  ylim(-1000000, 1000000) +
  labs(
    x = "Fitted value", y = "Residual",
    title = "Residuals vs. fitted values"
  )
```

## Residuals vs. fitted values

```{r}
#| echo: true
#| ref.label: "res-vs-fit"
#| fig.show: "hide"
```

## Application exercise

::: appex
ðŸ“‹ [github.com/sta210-s22/ae-3-duke-forest](https://github.com/sta210-s22/?q=ae-3-duke-forest&type=all&language=&sort=)
:::

```{r}
countdown(minutes = 5, font_size = "2em")
```

## Non-linear relationships

```{r}
set.seed(1234)

n = 100

df <- tibble(
  x = -49:50,
  e_curved = rnorm(n, 0, 150),
  y_curved = x^2 + e_curved,
  e_slight_curve = sort(rbeta(n, 5, 1) * 200) + rnorm(n, 0, 5),
  y_slight_curve = x + e_slight_curve,
  x_fan = seq(0, 3.99, 4 / n),
  y_fan = c(rnorm(n / 8, 3, 1), rnorm(n / 8, 3.5, 2), rnorm(n / 8, 4, 2.5), rnorm(n / 8, 4.5, 3), rnorm(n / 4, 5, 4), rnorm((n / 4) + 2, 6, 5))
)
```

::: columns
::: {.column width="50%"}
```{r}
#| out.width: "100%"
#| fig.asp: 1.2

p1 <- ggplot(df, aes(x = x, y = y_curved)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "#8F2D56") +
  labs(
    x = "X", y = "Y",
    title = "Observed data + model"
    )

curved_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(y_curved ~ x, data = df)

curved_aug <- augment(curved_fit$fit)

p2 <- ggplot(curved_aug, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(
    x = "Fitted value", y = "Residual",
    title = "Residuals vs. fitted values"
  ) +
  ylim(-2000, 2000)

p1 / p2 +
  plot_annotation(title = "Obviously curved")
```
:::

::: {.column width="50%"}
```{r}
#| out.width: "100%"
#| fig.asp: 1.2

p1 <- ggplot(df, aes(x = x, y = y_slight_curve)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "#8F2D56") +
  labs(
    x = "X", y = "Y",
    title = "Observed data + model"
    )

slight_curve_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(y_slight_curve ~ x, data = df)

slight_curve_aug <- augment(slight_curve_fit$fit)

p2 <- ggplot(slight_curve_aug, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(
    x = "Fitted value", y = "Residual",
    title = "Residuals vs. fitted values"
  )

p1 / p2 +
  plot_annotation(title = "Not so obviously curved")
```
:::
:::

## Constant variance

âœ… The vertical spread of the residuals should be relatively constant across the plot

```{r}
#| ref.label: "res-vs-fit"
```

## Non-constant variance

::: columns
::: {.column width="50%"}
```{r}
#| out.width: "100%"

ggplot(df, aes(x = x_fan, y = y_fan)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "#8F2D56") +
  labs(
    x = "X", y = "Y",
    title = "Observed data + model"
    )
```
:::

::: {.column width="50%"}
```{r}
#| out.width: "100%"

fan_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(y_fan ~ x_fan, data = df)

fan_aug <- augment(fan_fit$fit)

ggplot(fan_aug, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(
    x = "Fitted value", y = "Residual",
    title = "Residuals vs. fitted values"
  ) +
  ylim(-15, 15)
```
:::
:::

## Normality

```{r}
ggplot(df_aug, aes(x = .resid)) +
  geom_histogram(aes(y = ..density..), binwidth = 100000, color = "white") +
  geom_function(
    #geom = "line",
    fun = dnorm,
    args = list(
      mean = mean(df_aug$.resid), 
      sd = sd(df_aug$.resid)
      ),
    lwd = 2,
    col = "#8F2D5690"
  ) +
  labs(
    x = "Residual",
    y = "Density",
    title = "Histogram of residuals"
  )
```

## Independence

-   We can often check the independence assumption based on the context of the data and how the observations were collected

-   If the data were collected in a particular order, examine a scatterplot of the residuals versus order in which the data were collected

âœ… Based on available information, the error for one house does not tell us anything about the error for another use

## Recap

Used residual plots to check conditions for SLR:

::: columns
::: {.column width="50%"}
::: nonincremental
-   Linearity
-   Constant variance
:::
:::

::: {.column width="50%"}
::: nonincremental
-   Normality
-   Independence
:::
:::
:::

. . .

::: question
Which of these conditions are required for fitting a SLR?
Which for simulation-based inference for the slope for an SLR?
Which for inference with mathematical models?
:::

```{r}
countdown(minutes = 3, font_size = "2em")
```
